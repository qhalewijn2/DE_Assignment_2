{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "83828610-e913-4a5c-9969-873dfc7d0a1e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "            <div>\n",
       "                <p><b>SparkSession - in-memory</b></p>\n",
       "                \n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"http://f463436adf50:4041\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v3.5.0</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>spark://spark-master:7077</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>used_cars_stream</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        \n",
       "            </div>\n",
       "        "
      ],
      "text/plain": [
       "<pyspark.sql.session.SparkSession at 0x78521c3a1a10>"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark import SparkConf\n",
    "from pyspark.sql import functions as F\n",
    "from pyspark.sql.types import (\n",
    "    StructType, StructField,\n",
    "    StringType, IntegerType, DoubleType\n",
    ")\n",
    "\n",
    "# Create Spark configuration\n",
    "sparkConf = SparkConf()\n",
    "sparkConf.setMaster(\"spark://spark-master:7077\")\n",
    "sparkConf.setAppName(\"used_cars_stream\")\n",
    "sparkConf.set(\"spark.driver.memory\", \"2g\")\n",
    "sparkConf.set(\"spark.executor.cores\", \"1\")\n",
    "sparkConf.set(\"spark.driver.cores\", \"1\")\n",
    "\n",
    "# CRITICAL: Add Kafka packages\n",
    "sparkConf.set(\"spark.jars.packages\", \n",
    "              \"org.apache.spark:spark-sql-kafka-0-10_2.12:3.5.0\")\n",
    "\n",
    "# Create the spark session\n",
    "spark = SparkSession.builder.config(conf=sparkConf).getOrCreate()\n",
    "\n",
    "spark "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "13dc55e0-eede-4b1e-bd81-b6f0b30a4b9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#  Kafka settings \n",
    "KAFKA_BOOTSTRAP_SERVERS = \"kafka1:9093\"   # inside Docker network\n",
    "TOPIC_NAME = \"used_cars_stream\"        \n",
    "\n",
    "# JSON schema coming from the producer \n",
    "stream_schema = (\n",
    "    StructType()\n",
    "    .add(\"id\", StringType())\n",
    "    .add(\"url\", StringType())\n",
    "    .add(\"region\", StringType())\n",
    "    .add(\"price\", StringType())  # Will convert to double later\n",
    "    .add(\"year\", StringType())   # Will convert to int later\n",
    "    .add(\"manufacturer\", StringType())\n",
    "    .add(\"model\", StringType())\n",
    "    .add(\"condition\", StringType())\n",
    "    .add(\"fuel\", StringType())\n",
    "    .add(\"odometer\", StringType())  # Will convert to double later\n",
    "    .add(\"state\", StringType())\n",
    "    .add(\"posting_date\", StringType())\n",
    ")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c1fa2824-7444-45e0-8c4b-5c6fb52162b2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- id: string (nullable = true)\n",
      " |-- url: string (nullable = true)\n",
      " |-- region: string (nullable = true)\n",
      " |-- price: string (nullable = true)\n",
      " |-- year: string (nullable = true)\n",
      " |-- manufacturer: string (nullable = true)\n",
      " |-- model: string (nullable = true)\n",
      " |-- condition: string (nullable = true)\n",
      " |-- fuel: string (nullable = true)\n",
      " |-- odometer: string (nullable = true)\n",
      " |-- state: string (nullable = true)\n",
      " |-- posting_date: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 1) Read raw Kafka stream\n",
    "raw_df = (\n",
    "    spark.readStream\n",
    "         .format(\"kafka\")\n",
    "         .option(\"kafka.bootstrap.servers\", KAFKA_BOOTSTRAP_SERVERS)\n",
    "         .option(\"subscribe\", TOPIC_NAME)\n",
    "         .option(\"startingOffsets\", \"latest\")  \n",
    "         .load()\n",
    ")\n",
    "\n",
    "# 2) Extract JSON string from Kafka's value column\n",
    "json_df = raw_df.selectExpr(\"CAST(value AS STRING) AS json_string\")\n",
    "\n",
    "# 3) Parse JSON into columns\n",
    "parsed_df = (\n",
    "    json_df\n",
    "    .select(F.from_json(\"json_string\", stream_schema).alias(\"data\"))\n",
    "    .select(\"data.*\")\n",
    ")\n",
    "\n",
    "parsed_df.printSchema()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "463cff36-daff-4b61-8357-153b77c05bee",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- id: string (nullable = true)\n",
      " |-- url: string (nullable = true)\n",
      " |-- region: string (nullable = true)\n",
      " |-- price: double (nullable = true)\n",
      " |-- year: integer (nullable = true)\n",
      " |-- manufacturer: string (nullable = true)\n",
      " |-- model: string (nullable = true)\n",
      " |-- condition: string (nullable = true)\n",
      " |-- fuel: string (nullable = true)\n",
      " |-- odometer: double (nullable = true)\n",
      " |-- state: string (nullable = true)\n",
      " |-- posting_date: string (nullable = true)\n",
      " |-- state_clean: string (nullable = true)\n",
      " |-- posting_ts: timestamp (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# In Cell 5 - improve cleaning\n",
    "current_year = 2025\n",
    "\n",
    "df_clean = (\n",
    "    parsed_df\n",
    "    # Cast to proper types\n",
    "    .withColumn(\"price\", F.col(\"price\").cast(\"double\"))\n",
    "    .withColumn(\"year\", F.col(\"year\").cast(\"int\"))\n",
    "    .withColumn(\"odometer\", F.col(\"odometer\").cast(\"double\"))\n",
    "    \n",
    "    # Valid price\n",
    "    .filter(F.col(\"price\").isNotNull() & (F.col(\"price\") > 0) & (F.col(\"price\") < 200_000))\n",
    "    # Reasonable year\n",
    "    .filter(F.col(\"year\").isNotNull() & (F.col(\"year\") >= 1980) & (F.col(\"year\") <= current_year + 1))\n",
    "    # Odometer non-negative if present\n",
    "    .filter(F.col(\"odometer\").isNull() | (F.col(\"odometer\") >= 0))\n",
    ")\n",
    "\n",
    "# State cleaning\n",
    "df_clean = df_clean.withColumn(\n",
    "    \"state_clean\",\n",
    "    F.when(\n",
    "        (F.col(\"state\").isNotNull()) & (F.length(\"state\") <= 2),\n",
    "        F.upper(F.col(\"state\"))\n",
    "    )\n",
    ")\n",
    "\n",
    "# Event time\n",
    "df_clean = df_clean.withColumn(\n",
    "    \"posting_ts\",\n",
    "    F.to_timestamp(\"posting_date\")\n",
    ")\n",
    "\n",
    "df_clean.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "98f568cb-e2a8-4c28-a5a6-923e52918c8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Streaming aggregation per state and manufacturer\n",
    "agg_stream = (\n",
    "    df_clean\n",
    "    .groupBy(\"state_clean\", \"manufacturer\")\n",
    "    .agg(\n",
    "        F.count(\"*\").alias(\"listing_count\"),\n",
    "        F.avg(\"price\").alias(\"avg_price\"),\n",
    "        F.expr(\"percentile_approx(odometer, 0.5)\").alias(\"median_odometer\")\n",
    "    )\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "034718fa-e9be-4b11-8006-25866b3b8a05",
   "metadata": {},
   "outputs": [],
   "source": [
    "# DEBUG: Test if Kafka is readable\n",
    "from pyspark.sql import functions as F\n",
    "\n",
    "# Read just raw Kafka messages (no parsing)\n",
    "test_query = (\n",
    "    spark.readStream\n",
    "         .format(\"kafka\")\n",
    "         .option(\"kafka.bootstrap.servers\", \"kafka1:9093\")\n",
    "         .option(\"subscribe\", \"used_cars_stream\")\n",
    "         .option(\"startingOffsets\", \"earliest\")\n",
    "         .load()\n",
    "         .selectExpr(\"CAST(value AS STRING) as message\")\n",
    "         .writeStream\n",
    "         .format(\"console\")\n",
    "         .option(\"truncate\", \"false\")\n",
    "         .option(\"numRows\", 5)\n",
    "         .start()\n",
    ")\n",
    "\n",
    "test_query.awaitTermination(30)\n",
    "test_query.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "1defab4b-ac43-4bec-a174-f604f3b6c710",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Write stream to console\n",
    "query_console = (\n",
    "    agg_stream\n",
    "    .writeStream\n",
    "    .outputMode(\"complete\")       # re-emit full table each batch\n",
    "    .format(\"console\")\n",
    "    .option(\"truncate\", \"false\")\n",
    "    .option(\"numRows\", 20)\n",
    "    .start()\n",
    ")\n",
    "# Wait up to 30 seconds to see some batches\n",
    "query_console.awaitTermination(60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "1b47101e-8165-4a93-9e32-ad0ec6bc5049",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write cleaned stream to Parquet on disk\n",
    "output_path = \"/home/jovyan/data/used_cars_stream_agg\"\n",
    "checkpoint_path = \"/home/jovyan/checkpoint/used_cars_stream_agg\"\n",
    "\n",
    "\n",
    "query_parquet = (\n",
    "    df_clean\n",
    "    .writeStream\n",
    "    .outputMode(\"append\")\n",
    "    .format(\"parquet\")\n",
    "    .option(\"path\", output_path)\n",
    "    .option(\"checkpointLocation\", checkpoint_path)\n",
    "    .start()\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb0cdf14-90ad-4e16-a277-dcb3e1a202a8",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "stream_result = spark.read.parquet(\"/home/jovyan/data/used_cars_stream_agg\")\n",
    "stream_result.show(20, truncate=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a33c27a-6371-4adc-9fdd-bfb83dc8bd13",
   "metadata": {},
   "outputs": [],
   "source": [
    "# In a new cell\n",
    "try:\n",
    "    test_query.stop()\n",
    "except:\n",
    "    pass\n",
    "\n",
    "try:\n",
    "    query_console.stop()\n",
    "except:\n",
    "    pass\n",
    "\n",
    "spark.stop()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4689a4a8-0dd9-4a47-ac78-722c108f6245",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
